{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1df0fb3",
   "metadata": {},
   "source": [
    "## Cell 1: Install required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d334927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.35 datasets==2.13 sacrebleu sentencepiece gradio kaggle tensorflow==2.12 pyarrow==11.0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5fac12",
   "metadata": {},
   "source": [
    "## Cell 2: Imports and environment info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d6b3721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.13 (main, Aug  2 2025, 15:00:03) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "TensorFlow: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    T5TokenizerFast,\n",
    "    TFT5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "DATA_PATH = \"../data/medquad.csv\"\n",
    "MODEL_NAME = \"t5-small\"\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"TensorFlow:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1123027d",
   "metadata": {},
   "source": [
    "## Cell 3: Load MEDQA dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97b67705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded dataset with 16412 rows.\n",
      "                                 question  \\\n",
      "0                What is (are) Glaucoma ?   \n",
      "1                  What causes Glaucoma ?   \n",
      "2     What are the symptoms of Glaucoma ?   \n",
      "3  What are the treatments for Glaucoma ?   \n",
      "4                What is (are) Glaucoma ?   \n",
      "\n",
      "                                              answer           source  \\\n",
      "0  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n",
      "1  Nearly 2.7 million people have glaucoma, a lea...  NIHSeniorHealth   \n",
      "2  Symptoms of Glaucoma  Glaucoma can develop in ...  NIHSeniorHealth   \n",
      "3  Although open-angle glaucoma cannot be cured, ...  NIHSeniorHealth   \n",
      "4  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n",
      "\n",
      "  focus_area  \n",
      "0   Glaucoma  \n",
      "1   Glaucoma  \n",
      "2   Glaucoma  \n",
      "3   Glaucoma  \n",
      "4   Glaucoma  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"✅ Loaded dataset with {len(df)} rows.\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebf14e0",
   "metadata": {},
   "source": [
    "## Cell 4: Clean and Reformat Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aabce989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned and formatted dataset sample:\n",
      "                                         input_text  \\\n",
      "0                question: What is (are) Glaucoma ?   \n",
      "1                  question: What causes Glaucoma ?   \n",
      "2     question: What are the symptoms of Glaucoma ?   \n",
      "3  question: What are the treatments for Glaucoma ?   \n",
      "4                question: What is (are) Glaucoma ?   \n",
      "\n",
      "                                         target_text  \n",
      "0  Glaucoma is a group of diseases that can damag...  \n",
      "1  Nearly 2.7 million people have glaucoma, a lea...  \n",
      "2  Symptoms of Glaucoma  Glaucoma can develop in ...  \n",
      "3  Although open-angle glaucoma cannot be cured, ...  \n",
      "4  Glaucoma is a group of diseases that can damag...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = df.dropna(subset=[\"question\", \"answer\"])\n",
    "\n",
    "# Strip whitespace\n",
    "df[\"question\"] = df[\"question\"].astype(str).str.strip()\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip()\n",
    "\n",
    "# Reformat into T5 input/output pairs\n",
    "df[\"input_text\"] = \"question: \" + df[\"question\"]\n",
    "df[\"target_text\"] = df[\"answer\"]\n",
    "\n",
    "print(\"✅ Cleaned and formatted dataset sample:\")\n",
    "print(df[[\"input_text\", \"target_text\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964eae05",
   "metadata": {},
   "source": [
    "## Cell 5: Split into Train and Test Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7a56210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training samples: 14766\n",
      "✅ Testing samples: 1641\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"✅ Training samples: {len(train_df)}\")\n",
    "print(f\"✅ Testing samples: {len(test_df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d08834",
   "metadata": {},
   "source": [
    "## Cell 6: Initialize Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2692843d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\n",
    "print(\"✅ Tokenizer loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2327b7f",
   "metadata": {},
   "source": [
    "## Cell 7: Tokenization Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0703854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing function ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        examples[\"target_text\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"✅ Preprocessing function ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa2c53",
   "metadata": {},
   "source": [
    "## Cell 8: Convert to Hugging Face Dataset Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5f6d57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenization completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = HFDataset.from_pandas(train_df)\n",
    "test_dataset = HFDataset.from_pandas(test_df)\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "print(\"✅ Tokenization completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577eb2a1",
   "metadata": {},
   "source": [
    "## Convert to TensorFlow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b879d0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow datasets ready for model training!\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# Data collator handles dynamic padding and label alignment automatically\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
    "\n",
    "# Convert tokenized datasets to TensorFlow datasets\n",
    "train_dataset_tf = model.prepare_tf_dataset(\n",
    "    tokenized_train,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "test_dataset_tf = model.prepare_tf_dataset(\n",
    "    tokenized_test,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "print(\"✅ TensorFlow datasets ready for model training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c6ed9",
   "metadata": {},
   "source": [
    "## Cell 9: Import Training Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6667d167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import TFT5ForConditionalGeneration, create_optimizer\n",
    "import math\n",
    "\n",
    "MODEL_PATH = \"t5-small\"  \n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "# Load pre-trained model\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(MODEL_PATH)\n",
    "print(\"✅ Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d86131",
   "metadata": {},
   "source": [
    "## Cell 10: Create Optimizer and Compile Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9a915b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model compiled and ready for training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "steps_per_epoch = len(train_df) // BATCH_SIZE\n",
    "num_train_steps = steps_per_epoch * EPOCHS\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=LEARNING_RATE,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    ")\n",
    "\n",
    "model.compile(optimizer=optimizer)\n",
    "print(\"✅ Model compiled and ready for training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e486ba58",
   "metadata": {},
   "source": [
    "## Cell 11: Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d4f0378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1845/1845 [==============================] - 7066s 4s/step - loss: 2.6934 - val_loss: 2.1201\n",
      "Epoch 2/3\n",
      "1845/1845 [==============================] - 7597s 4s/step - loss: 2.2375 - val_loss: 1.9896\n",
      "Epoch 3/3\n",
      "1845/1845 [==============================] - 6954s 4s/step - loss: 2.1198 - val_loss: 1.9136\n",
      "✅ Training completed successfully. Best model saved at: ./models/t5_medqa_finetuned_best\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 5e-5\n",
    "checkpoint_path = \"./models/t5_medqa_finetuned_best\"\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Compile without setting loss manually\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    train_dataset_tf,\n",
    "    validation_data=test_dataset_tf,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "print(\"✅ Training completed successfully. Best model saved at:\", checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf940c6",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95e5a6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and tokenizer successfully saved to: ./models/t5_finetuned_helpdesk_v2\n"
     ]
    }
   ],
   "source": [
    "# Step: Save fine-tuned model and tokenizer\n",
    "\n",
    "SAVE_DIR = \"./models/t5_finetuned_helpdesk_v2\"  # choose any folder name you like\n",
    "\n",
    "# ✅ Create the directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ✅ Save the model and tokenizer\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(f\"✅ Model and tokenizer successfully saved to: {SAVE_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
